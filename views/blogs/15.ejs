<!DOCTYPE html>
<html>

<head>
  <% include ../partials/header.ejs%>
  <link href = "/css/common.css" rel = "stylesheet" />
  <link href = "/css/project/common.css" rel = "stylesheet" />
  <link href = "/css/start.css" rel="stylesheet" />
  <link href = "/css/icons.css" rel="stylesheet" />
  <link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
</head>

<body>
  <% include ../partials/navbar %>

  <h1>Audio Classification, Visualization and New York</h1>
  <h4>August 30th, 2020</h4>

  <center><img src = "/images/tusk.jpg" /></center>

  <div id = "mid">
    <p>
      Towards the end of the summer, my good friend Cody Benkoski and I were on a phone call. Cody had been working with Machine Learning algorithms for a while and had just completed an enormous project, creating an AI to classify Shiba dogs. You can find that project here. I loved the potential of big data and was eager to find out more about the practice. The next day, I found a hackathon competition where the goal was to use the Amazon Web Service Data Exchange to create cool data science projects. I called Cody, and we began work on Project Tusk.
      <br/><br/>
      Because AWS Data Exchange has such a huge collection of information, we took our time deciding which route to go down. We knew we wanted to focus on health, and we knew we wanted to pick a project that would be helpful to a large portion of the population. While doing research, I came across a New York Times article talking about how quiet New York City had become during the pandemic. Looking further into the matter, I saw that my generation was likely to have hearing disabilities sooner in life than any other generation prior. I talked it over with Cody, and we decided to look into the matter. The Data Exchange had information directly from the CDC showing the prevalence of hearing disability in the country, partitioned by state and age group. Plotting this on a graph, we saw that for people ages 18-44, a significant amount of the population suffered from hearing disability.
      <br/><br/>
      The research around hearing loss tends to be clear. An enormous amount of hearing disability is caused by exposure to loud noises for a prolonged period of time. With that in mind, we wanted to find a data driven way to showcase the dangers of loud sounds. I reached out to New York Times Reporter Emily Badger, the person whose piece had started this project. She recommended we look at the SONYC data set. SONYC is a project conducted by NYU to track noise pollution and other issues relating to sound in New York. This was a major leap, as it gave us data we could now analyze. Writing scripts to clean the data, we then decided to plot the data on a map. SONYC works by sending microphones out into the city to record normal city noise. We plotted where each of those microphones were on a map and had them pulse to showcase the relative volume of their noises. This representation made it easy for people to understand that the noise can have a serious impact on you. Then we used a marker to help people connect to the mobile part of the data. Moving the cursor around the map helps people see the ways that you can come into contact with the loud noises without even thinking you’re that close.
      <br/><br/>
      Finally, we wanted to see how one might go about solving this problem of hearing loss with machine learning. One possibility we entertained was software in hearing aids that would tune out loud noises. The catch was that some loud noises must be loud. A car horn or a police siren indicate danger and should almost always be heard by the user. Thus, we used the Convolutional Neural Network (CNN) devised by SONYC researchers to test how that could be done. Our conclusion was that these CNN have to be focused on getting the percentages right rather than getting the one right answer. That’s because when classifying audio data, multiple true classes could be present. You could hear both an engine idling and a car horn going. The CNN has to know that both are present so the user can be aware.
      <br/><br/>
      All in all, the project allowed me to see new technologies that are incredibly promising. The ease with which you can find phenomenal data will only increase as more fantastic projects like SONYC get funding. In short, the project was a great way to see how the world can both start to recognize enormous scale problems and solve them one sound at a time.

      Onwards & Upwards
    </p>
  </div>

    <% include ../partials/footer.ejs%>


</body>
